package org.kson.parser

import org.kson.parser.TokenType.*
import org.kson.parser.messages.MessageType
import org.kson.parser.messages.MessageType.*
import kotlin.test.Test
import kotlin.test.assertEquals
import kotlin.test.assertFalse

class LexerTest {
    /**
     * Assertion helper for testing [source] produces the sequence of [expectedTokenTypes].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenTypes is the list of expected types for the resulting tokens
     * @param message optionally pass a custom failure message for this assertion
     *
     * Returns the list of whole [Token]s for further validation
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenTypes: List<TokenType>,
        message: String? = null,
        testGapFreeLexing: Boolean = false
    ): List<Token> {
        val messageSink = MessageSink()
        val actualTokens = Lexer(source, messageSink, testGapFreeLexing).tokenize()
        val actualTokenTypes = actualTokens.map { it.tokenType }.toMutableList()

        assertFalse(
            messageSink.hasErrors(),
            "Should not have lexing errors, got:\n\n" + LoggedMessage.print(messageSink.loggedMessages())
        )
        assertEquals(
            expectedTokenTypes,
            actualTokenTypes,
            message
        )

        return actualTokens
    }

    /**
     * Assertion helper to validate [Location]s generated by [Lexer].
     *
     * This test ensures that lexing [source] produces a sequence of tokens that matches the
     * given [expectedTokenLocationPairs].
     *
     * @param source is the kson source to tokenize
     * @param expectedTokenLocationPairs a list of [TokenType]/[Location] pairs that, by position, must match the
     *                                   [TokenType]/[Location] pairs produced by tokenizing [source]
     */
    private fun assertTokenizesTo(
        source: String,
        expectedTokenLocationPairs: List<Pair<TokenType, Location>>,
        testGapFreeLexing: Boolean = false
    ) {
        val tokens = assertTokenizesTo(source, expectedTokenLocationPairs.map { it.first }, null, testGapFreeLexing)
        expectedTokenLocationPairs.forEachIndexed { index, tokenLocationPair ->
            val (tokenType, location) = tokenLocationPair
            assertEquals(
                location,
                tokens[index].lexeme.location,
                "Incorrect location for token of type $tokenType at index $index of the lexed tokens\n"
            )
        }
    }

    /**
     * Assertion helper for testing that tokenizing [source] generates [expectedMessageTypes].
     *
     * Returns the generated tokens for further validation
     */
    private fun assertTokenizesWithMessages(source: String, expectedMessageTypes: List<MessageType>): List<Token> {
        val messageSink = MessageSink()
        val tokens = Lexer(source, messageSink).tokenize()

        assertEquals(expectedMessageTypes, messageSink.loggedMessages().map { it.message.type })
        return tokens
    }

    @Test
    fun testEmptySource() {
        assertTokenizesTo(
            "",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            " ",
            emptyList<TokenType>()
        )

        assertTokenizesTo(
            "\t\n",
            emptyList<TokenType>()
        )
    }

    /**
     * Ensure we're well-behaved on source which has no leading or trailing whitespace
     */
    @Test
    fun testNoWhitespaceSource() {
        assertTokenizesTo(
            "1",
            listOf(NUMBER)
        )
    }

    @Test
    fun testStringLiteralSource() {
        assertTokenizesTo(
            """
                "This is a string"
            """,
            listOf(STRING)
        )
    }

    @Test
    fun testNumberLiteralSource() {
        assertTokenizesTo(
            """
                42
                42E0
                42e0
                4.2E1
                420E-1
                4200e-2
                0.42e2
                0.42e+2
                42E+0
                00042E0
                -42
                -42E0
                -42e0
                -4.2E1
                -420E-1
                -4200e-2
                -0.42e2
                -0.42e+2
                -42E+0
                -00042E0
            """,
            listOf(
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER,
                NUMBER
            )
        )

        // error case
        assertTokenizesWithMessages(
            """
                420E
            """,
            listOf(DANGLING_EXP_INDICATOR)
        )

        // error case
        assertTokenizesWithMessages(
            """
                420E-
            """,
            listOf(DANGLING_EXP_INDICATOR)
        )
    }

    @Test
    fun testDanglingMinusSign() {
        assertTokenizesWithMessages(
            """
                -nope
            """,
            listOf(ILLEGAL_MINUS_SIGN)
        )
    }

    @Test
    fun testBooleanLiteralSource() {
        assertTokenizesTo(
            """
                true
            """,
            listOf(TRUE)
        )

        assertTokenizesTo(
            """
                false
            """,
            listOf(FALSE)
        )
    }

    @Test
    fun testNilLiteralSource() {
        assertTokenizesTo(
            """
                null
            """,
            listOf(NULL)
        )
    }

    @Test
    fun testEmptyListSource() {
        assertTokenizesTo(
            """
                []
            """,
            listOf(BRACKET_L, BRACKET_R)
        )
    }

    @Test
    fun testBracketListSource() {
        assertTokenizesTo(
            """
                ["a string"]
            """,
            listOf(BRACKET_L, STRING, BRACKET_R)
        )

        assertTokenizesTo(
            """
                [42, 43, 44]
            """,
            listOf(BRACKET_L, NUMBER, COMMA, NUMBER, COMMA, NUMBER, BRACKET_R)
        )
    }

    @Test
    fun testDashListSource() {
        // ensure we test the boundary when `-` is the first char in the source (not whitespace, for instance)
        assertTokenizesTo(
            "- null",
            listOf(LIST_DASH, NULL)
        )

        assertTokenizesTo(
            """
                - "a string"
            """,
            listOf(LIST_DASH, STRING)
        )

        assertTokenizesTo(
            """
                - 42
                - 43
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )

        // odd but kind of needs to be legal to keep the dash list element semantics straightforward...
        // a dash-list element is simply: a dash followed by a value
        assertTokenizesTo(
            """
                - 42 - 43 - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, NUMBER, LIST_DASH, NUMBER)
        )
    }

    @Test
    fun testMixedLists() {
        assertTokenizesTo(
            """
                - 42
                - [2, 4]
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, BRACKET_L, NUMBER, COMMA, NUMBER, BRACKET_R, LIST_DASH, NUMBER)
        )

        // this must lex in spite of the fact it will parse with errors on the illegal list nesting
        assertTokenizesTo(
            """
                - 42
                - 
                    - "nope, not a sublist because it's ambiguous"
                    - "indentation is not significant!"
                - 44
            """,
            listOf(LIST_DASH, NUMBER, LIST_DASH, LIST_DASH, STRING, LIST_DASH, STRING, LIST_DASH, NUMBER)
        )
    }

    @Test
    fun testEmptyObjectSource() {
        assertTokenizesTo(
            """
                {}
            """,
            listOf(BRACE_L, BRACE_R)
        )
    }

    @Test
    fun testObjectSource() {
        assertTokenizesTo(
            """
                {
                    key: val
                    "string key": 66
                    hello: "y'all"
                }
            """,
            listOf(BRACE_L, IDENTIFIER, COLON, IDENTIFIER, STRING, COLON, NUMBER, IDENTIFIER, COLON, STRING, BRACE_R)
        )

        assertTokenizesTo(
            """
                key: val
                "string key": 66
                hello: "y'all"
            """,
            listOf(IDENTIFIER, COLON, IDENTIFIER, STRING, COLON, NUMBER, IDENTIFIER, COLON, STRING)
        )
    }

    @Test
    fun testComments() {
        assertTokenizesTo(
            """
                # a comment!
                key: val
                        # wahoo!  Another comment!
                     # follow-up comment with wacky indent ðŸ•º
                "string key": 66
                hello: "y'all"
            """,
            listOf(
                COMMENT,
                IDENTIFIER,
                COLON,
                IDENTIFIER,
                COMMENT,
                COMMENT,
                STRING,
                COLON,
                NUMBER,
                IDENTIFIER,
                COLON,
                STRING
            )
        )
    }

    @Test
    fun testEmbedBlockSource() {
        assertTokenizesTo(
            """
                %%
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertTokenizesTo(
            """
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )
    }

    @Test
    fun testEmbedBlockIndentTrimming() {
        val oneLineEmbedTokens = assertTokenizesTo(
            """
                %%
                this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("this is a raw embed\n", oneLineEmbedTokens[1].value)

        val mulitLineEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """
                this is a multi-line
                    raw embed
            who's indent will be determined by
                            the leftmost line
            
            """.trimIndent(),
            mulitLineEmbedTokens[2].value
        )

        val mulitLineIndentedEmbedTokens = assertTokenizesTo(
            """
                %%sql
                    this is a multi-line
                        raw embed
                who's indent will be determined by
                                the leftmost line,
                which is the end delimiter in this case
              %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """      this is a multi-line
          raw embed
  who's indent will be determined by
                  the leftmost line,
  which is the end delimiter in this case
""",
            mulitLineIndentedEmbedTokens[2].value
        )
    }

    @Test
    fun testEmbedBlockTrialingWhitespace() {
        val trailingNewlineTokens = assertTokenizesTo(
            """
                %%
                this should have a newline at the end
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("this should have a newline at the end\n", trailingNewlineTokens[1].value)

        val trailingSpacesTokens = assertTokenizesTo(
            """
                %%
                this lovely embed
                    should have four trailing 
                    spaces and a newline at the end    
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            """
            this lovely embed
                should have four trailing 
                spaces and a newline at the end    

            """.trimIndent(),
            trailingSpacesTokens[1].value
        )

        val zeroTrailingWhitespaceTokens = assertTokenizesTo(
            """
                %%
                    this on the other hand,
                    should have spaces but no newline at the end    %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals(
            "this on the other hand,\nshould have spaces but no newline at the end    ",
            zeroTrailingWhitespaceTokens[1].value
        )
    }

    @Test
    fun testEmbedBlockTrailingWhitespace() {
        assertTokenizesTo(
            // note the extra whitespace after the opening `%%`
            """
                %%   
                    this is a raw embed
                %%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END),
            "should allow trailing whitespace after the opening '%%'"
        )

        assertTokenizesTo(
            // note the extra whitespace after the opening `%%`
            """   
                %%sql
                    select * from something
                %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END),
            "should allow trailing whitespace after the opening '%%embedTag'"
        )
    }

    @Test
    fun testEmbedBlockDanglingDelim() {
        assertTokenizesWithMessages(
            """
            test: %
            """,
            listOf(EMBED_BLOCK_DANGLING_DELIM)
        )
    }

    @Test
    fun testComplexEmbedTagWithWhitespace() {
        assertTokenizesTo(
            """
            %%   this tag has spaces and funky characters ~!@#$%^&*()_+
            some sweet content
            %%
            """,
            listOf(EMBED_START, EMBED_TAG, EMBED_CONTENT, EMBED_END)
        )
    }

    @Test
    fun testUnclosedEmbedBlock() {
        assertTokenizesTo(
            """
            %%
            This embed block lacks its closing delimiter
            """,
            listOf(EMBED_START, EMBED_CONTENT)
        )
    }

    @Test
    fun testUnterminatedString() {
        val unclosedStringTokens = assertTokenizesWithMessages(
            """
            "this string has no end quote
            """,
            listOf(STRING_NO_CLOSE)
        )
        assertEquals(listOf(STRING), unclosedStringTokens.map { it.tokenType })
    }

    @Test
    fun testUnterminatedAltString() {
        val unclosedStringTokens = assertTokenizesWithMessages(
            """
            'this string has no end quote
            """,
            listOf(STRING_NO_CLOSE)
        )
        assertEquals(listOf(STRING), unclosedStringTokens.map { it.tokenType })
    }

    @Test
    fun testIdentifierLexemeContent() {
        val tokens = assertTokenizesTo(
            """   
                a_key: "a_value"
            """,
            listOf(IDENTIFIER, COLON, STRING)
        )

        assertEquals("a_key", tokens[0].value)
        assertEquals("a_value", tokens[2].value)
    }

    @Test
    fun testTokenLocations() {
        assertTokenizesTo(
            """
            |{
            |    key: val
            |    list: [true, false]
            |    embed: %%
            |      multiline tokens
            |      should have correct
            |      Locations too
            |      %%
            |}
            """.trimMargin(),
            listOf(
                Pair(BRACE_L, Location(0, 0, 0, 1, 0, 1)),
                Pair(IDENTIFIER, Location(1, 4, 1, 7, 6, 9)),
                Pair(COLON, Location(1, 7, 1, 8, 9, 10)),
                Pair(IDENTIFIER, Location(1, 9, 1, 12, 11, 14)),
                Pair(IDENTIFIER, Location(2, 4, 2, 8, 19, 23)),
                Pair(COLON, Location(2, 8, 2, 9, 23, 24)),
                Pair(BRACKET_L, Location(2, 10, 2, 11, 25, 26)),
                Pair(TRUE, Location(2, 11, 2, 15, 26, 30)),
                Pair(COMMA, Location(2, 15, 2, 16, 30, 31)),
                Pair(FALSE, Location(2, 17, 2, 22, 32, 37)),
                Pair(BRACKET_R, Location(2, 22, 2, 23, 37, 38)),
                Pair(IDENTIFIER, Location(3, 4, 3, 9, 43, 48)),
                Pair(COLON, Location(3, 9, 3, 10, 48, 49)),
                Pair(EMBED_START, Location(3, 11, 3, 13, 50, 52)),
                Pair(EMBED_CONTENT, Location(4, 0, 7, 6, 53, 128)),
                Pair(EMBED_END, Location(7, 6, 7, 8, 128, 130)),
                Pair(BRACE_R, Location(8, 0, 8, 1, 131, 132))
            )
        )
    }

    @Test
    fun testStringEscapes() {
        val tokens = assertTokenizesTo(
            """   
                "string with 'unescaped' and \"embedded\" quotes"
            """,
            listOf(STRING)
        )

        assertEquals("string with 'unescaped' and \"embedded\" quotes", tokens[0].value)
    }

    @Test
    fun testAltStringEscapes() {
        val tokens = assertTokenizesTo(
            """
                'string with "unescaped" and \'embedded\' quotes'
            """,
            listOf(STRING)
        )

        assertEquals("string with \"unescaped\" and \'embedded\' quotes", tokens[0].value)
    }

    @Test
    fun testEmbeddedBlockDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                %%
                these double %\% percents are embedded but escaped%%
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("these double %% percents are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testEmbeddedBlockAltDelimiterEscapes() {
        val singleEscapeTokens = assertTokenizesTo(
            """   
                $$
                these double $\$ dollars are embedded but escaped$$
            """,
            listOf(EMBED_START, EMBED_CONTENT, EMBED_END)
        )

        assertEquals("these double $$ dollars are embedded but escaped", singleEscapeTokens[1].value)
    }

    @Test
    fun testGapFreeLexing() {
        assertTokenizesTo(
            """
                key: val
            """,
            listOf(WHITESPACE, IDENTIFIER, COLON, WHITESPACE, IDENTIFIER, WHITESPACE),
            "Should include WHITESPACE tokens when lexing gap-free",
            true
        )

        assertTokenizesTo(
            """
                |  quoted: "string"
                |
            """.trimMargin(),
            listOf(
                Pair(WHITESPACE, Location(0, 0, 0, 2, 0, 2)),
                Pair(IDENTIFIER, Location(0, 2, 0, 8, 2, 8)),
                Pair(COLON, Location(0, 8, 0, 9, 8, 9)),
                Pair(WHITESPACE, Location(0, 9, 0, 10, 9, 10)),
                Pair(STRING, Location(0, 10, 0, 18, 10, 18)),
                Pair(WHITESPACE, Location(0, 18, 1, 0, 18, 19))
            ),
            true
        )
    }
}